{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Search: Playing Dots and Boxes\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Total Points: Undegraduates 100, graduate students 110\n",
    "\n",
    "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You will implement different versions of agents that play the game Dots and Boxes:\n",
    "\n",
    "> \"Dots and Boxes is a pencil-and-paper game for two players. The game starts with an empty grid of dots. Usually two players take turns adding a single horizontal or vertical line between two unjoined adjacent dots. A player who completes the fourth side of a 1x1 box earns one point and takes another turn. A point is typically recorded by placing a mark that identifies the player in the box, such as an initial. The game ends when no more lines can be placed. The winner is the player with the most points. The board may be of any size grid.\" (see [Dots and Boxes on Wikipedia](https://en.wikipedia.org/wiki/Dots_and_Boxes))\n",
    "\n",
    "You can play Dots and Boxes [here](https://www.math.ucla.edu/~tom/Games/dots&boxes.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Defining the Search Problem [10 point]\n",
    "\n",
    "Define the components of the search problem associated with this game:\n",
    "\n",
    "* Initial state\n",
    "* Actions\n",
    "* Transition model\n",
    "* Test for the terminal state\n",
    "* Utility for terminal states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/answer goes here.\n",
    "# --- Định nghĩa các thành phần của bài toán tìm kiếm ---\n",
    "\n",
    "# 1. Trạng thái ban đầu (Initial state):\n",
    "#   - Bàn chơi là một lưới các điểm, ban đầu chưa có đường nào được kẻ.\n",
    "#   - Ví dụ với bàn N×N ô vuông, có tổng cộng 2*N*(N+1) cạnh (gồm cạnh ngang và dọc).\n",
    "#   - Ban đầu tất cả cạnh đều chưa được chọn, người chơi 1 bắt đầu trước.\n",
    "\n",
    "initial_state = {\n",
    "    \"edges\": set(),       # chưa có cạnh nào được vẽ\n",
    "    \"player\": 1,          # người chơi 1 đi trước\n",
    "    \"scores\": {1: 0, 2: 0}\n",
    "}\n",
    "\n",
    "# 2. Hành động (Actions):\n",
    "#   - Một hành động là việc chọn một cạnh (đường ngang hoặc dọc)\n",
    "#     giữa hai điểm liền kề chưa được nối.\n",
    "#   - Mỗi lần chỉ chọn 1 cạnh.\n",
    "\n",
    "def get_actions(state, all_edges):\n",
    "    return list(all_edges - state[\"edges\"])\n",
    "\n",
    "# 3. Mô hình chuyển trạng thái (Transition model):\n",
    "#   - Khi một người chơi chọn một cạnh:\n",
    "#       + Nếu cạnh đó hoàn thành 1 hoặc nhiều ô vuông, người chơi đó được điểm và được đi tiếp.\n",
    "#       + Nếu không hoàn thành ô nào, lượt chơi chuyển sang người còn lại.\n",
    "\n",
    "def transition_model(state, action, all_boxes):\n",
    "    new_state = state.copy()\n",
    "    new_state[\"edges\"] = state[\"edges\"] | {action}\n",
    "    completed_boxes = [\n",
    "        b for b in all_boxes if set(b).issubset(new_state[\"edges\"])\n",
    "        and not set(b).issubset(state[\"edges\"])\n",
    "    ]\n",
    "    if completed_boxes:\n",
    "        new_state[\"scores\"][state[\"player\"]] += len(completed_boxes)\n",
    "    else:\n",
    "        new_state[\"player\"] = 3 - state[\"player\"]  # đổi lượt (1 <-> 2)\n",
    "    return new_state\n",
    "\n",
    "# 4. Kiểm tra trạng thái kết thúc (Terminal test):\n",
    "#   - Trò chơi kết thúc khi tất cả các cạnh đều đã được chọn.\n",
    "\n",
    "def is_terminal(state, all_edges):\n",
    "    return len(state[\"edges\"]) == len(all_edges)\n",
    "\n",
    "# 5. Giá trị tiện ích (Utility) của trạng thái kết thúc:\n",
    "#   - Độ hữu ích được tính bằng hiệu số điểm giữa người chơi 1 và người chơi 2.\n",
    "\n",
    "def utility(state):\n",
    "    return state[\"scores\"][1] - state[\"scores\"][2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the state space? Give an estimate and explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Với bàn N×N ô vuông:\n",
    "#   - Số cạnh có thể chọn E = 2*N*(N+1)\n",
    "#   - Mỗi cạnh có thể \"được chọn\" hoặc \"chưa chọn\"\n",
    "#   → Số trạng thái có thể có xấp xỉ = 2^E = 2^(2*N*(N+1))\n",
    "\n",
    "# Ví dụ:\n",
    "#   N = 2  → E = 12 → 2^12 = 4096 trạng thái\n",
    "#   N = 3  → E = 24 → 2^24 ≈ 16.7 triệu trạng thái\n",
    "#\n",
    "# Tuy nhiên, không phải tất cả các trạng thái đều hợp lệ (vì có trạng thái không thể đạt được),\n",
    "# nhưng nhìn chung độ phức tạp vẫn tăng theo hàm mũ.\n",
    "#\n",
    "# → Kích thước không gian trạng thái: O(2^(2*N*(N+1)))\n",
    "\n",
    "state_space_estimate = \"O(2^(2*N*(N+1)))\"\n",
    "explanation = (\n",
    "    \"Không gian trạng thái tăng theo hàm mũ theo số cạnh, \"\n",
    "    \"vì vậy việc duyệt toàn bộ trạng thái là không khả thi với bàn lớn.\"\n",
    ")\n",
    "state_space_estimate, explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How big is the game tree that minimax search will go through? Give an estimate and explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Trong thuật toán minimax:\n",
    "#   - Ở mỗi tầng, người chơi chọn một cạnh chưa được vẽ.\n",
    "#   - Số nhánh (branching factor) ≈ số cạnh còn lại.\n",
    "#   - Độ sâu của cây = tổng số cạnh E = 2*N*(N+1)\n",
    "#\n",
    "# Như vậy, số lá (số ván có thể có) xấp xỉ E! (giai thừa của số cạnh),\n",
    "# vì mỗi lượt chọn một cạnh khác nhau cho đến khi không còn cạnh nào.\n",
    "#\n",
    "# Ví dụ:\n",
    "#   N = 2 → E = 12 → 12! ≈ 4.8 × 10^8 nút lá\n",
    "#\n",
    "# Cây trò chơi tăng siêu cấp theo hàm giai thừa,\n",
    "# nên việc áp dụng minimax đầy đủ là không khả thi trừ khi bàn rất nhỏ.\n",
    "#\n",
    "# → Kích thước cây trò chơi: O(E!) với E = 2*N*(N+1)\n",
    "\n",
    "game_tree_estimate = \"O(E!) where E = 2*N*(N+1)\"\n",
    "explanation_tree = (\n",
    "    \"Cây trò chơi phát triển theo giai thừa của số cạnh, \"\n",
    "    \"khiến minimax chỉ khả thi với các bàn nhỏ (ví dụ 1x1 hoặc 2x2).\"\n",
    ")\n",
    "game_tree_estimate, explanation_tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Game Environment and Random Agent [30 point]\n",
    "\n",
    "You need to think about a data structure to represent the board meaning he placed lines and who finished what box. There are many options. Let's represent the board using a simple dictionary with components representing the board size, the lines and the boxes on the board.\n",
    "\n",
    "**Important:** Everybody needs to use the same representation so we can let agents play against each other later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'size': (4, 4),\n",
       " 'lines': {('h', 1, 1): True, ('v', 1, 1): True},\n",
       " 'boxes': dict}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board = {\n",
    "    'size': (4, 4),  ### number of rows and columns of dots\n",
    "    'lines': dict(), ### keys are the set of drawn lines\n",
    "    'boxes': dict    ### keys are the boxes and the value is the player who completed each box\n",
    "}\n",
    "\n",
    "def draw_line(board, orientation, row, col):\n",
    "    \"\"\"\n",
    "    Place a line on an exiting board.\n",
    "       \n",
    "    Parameters\n",
    "    ----------\n",
    "    board: dict\n",
    "        the board\n",
    "    orientation: str\n",
    "        either 'h' or 'v' for horizontal or vertical\n",
    "    row, col: int\n",
    "        index of the starting dot for the line (starting with 0)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if orientation not in ['h', 'v']:\n",
    "        return False\n",
    "        \n",
    "    if row < 0 or col < 0:\n",
    "        return False\n",
    "        \n",
    "    if row >= board['size'][0] + (orientation == 'v') or col >= board['size'][1] + (orientation == 'h'):\n",
    "        return False\n",
    "        \n",
    "    if (orientation, row, col) in board['lines']:\n",
    "        return False\n",
    "            \n",
    "    board[\"lines\"][(orientation, row, col)] = True\n",
    "    return True\n",
    "    \n",
    "\n",
    "print(draw_line(board, \"h\", 1, 1))\n",
    "print(draw_line(board, \"v\", 1, 1))\n",
    "\n",
    "# this should not work\n",
    "print(draw_line(board, \"h\", 1, 1))\n",
    "\n",
    "board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to display the board. **Bonus point: Post your visualization code with an example output to the discussion board. The best visualization will earn you bonus participation points in this class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_board(board):\n",
    "    \"\"\"\n",
    "    Vẽ bàn chơi Dots and Boxes hiện tại.\n",
    "    Các điểm được biểu diễn bằng chấm tròn.\n",
    "    Các cạnh được kẻ giữa các điểm.\n",
    "    Các ô hoàn thành sẽ có màu theo người chơi.\n",
    "    \"\"\"\n",
    "    rows, cols = board['size']\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xticks(range(cols + 1))\n",
    "    ax.set_yticks(range(rows + 1))\n",
    "    ax.grid(True)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Vẽ các đường đã được chọn\n",
    "    for (orientation, r, c) in board['lines']:\n",
    "        if orientation == 'h':\n",
    "            ax.plot([c, c + 1], [r, r], color='black', linewidth=2)\n",
    "        elif orientation == 'v':\n",
    "            ax.plot([c, c], [r, r + 1], color='black', linewidth=2)\n",
    "\n",
    "    # Tô màu các ô đã hoàn thành\n",
    "    for (r, c), player in board['boxes'].items():\n",
    "        color = 'lightblue' if player == 1 else 'lightcoral'\n",
    "        ax.add_patch(plt.Rectangle((c, r), 1, 1, color=color, alpha=0.5))\n",
    "        ax.text(c + 0.5, r + 0.5, str(player), va='center', ha='center', fontsize=12, weight='bold')\n",
    "\n",
    "    plt.title(\"Dots and Boxes\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement helper functions for:\n",
    "\n",
    "* The transition model $result(s, a)$.\n",
    "* The utility function $utility(s)$.\n",
    "* Check for terminal states $terminal(s)$.\n",
    "* A check for available actions in each state $actions(s)$.\n",
    "\n",
    "__Notes:__\n",
    "* Make sure that all these functions work with boards of different sizes (number of columns and rows as stored in the board).\n",
    "* The result function updates the board and evaluates if the player closed a box and needs to store that information on the board. Add elements of the form `(row,col): player` to the board dictionary. `row` and `col` are the coordinates for the box and `player` is +1 or -1 representing the player. For example `(0,0): -1` means that the top-left box belongs to the other player. \n",
    "* _Important:_ Remember that a player goes again after she completes a box!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "def result(board, action, player):\n",
    "    \"\"\"\n",
    "    Cập nhật bàn khi người chơi thực hiện hành động (kẻ một cạnh).\n",
    "    Nếu người chơi hoàn thành được ô vuông nào thì được đi tiếp.\n",
    "    \"\"\"\n",
    "    orientation, row, col = action\n",
    "\n",
    "    # Nếu hành động không hợp lệ → không thay đổi\n",
    "    if not draw_line(board, orientation, row, col):\n",
    "        return board, player\n",
    "\n",
    "    rows, cols = board['size']\n",
    "    boxes_completed = []\n",
    "\n",
    "    # Kiểm tra các ô có thể bị ảnh hưởng\n",
    "    if orientation == 'h':\n",
    "        adjacent = [(row - 1, col), (row, col)]\n",
    "    else:\n",
    "        adjacent = [(row, col - 1), (row, col)]\n",
    "\n",
    "    # Kiểm tra xem ô đó có đủ 4 cạnh chưa\n",
    "    for (r, c) in adjacent:\n",
    "        if 0 <= r < rows and 0 <= c < cols:\n",
    "            edges_needed = {\n",
    "                ('h', r, c),\n",
    "                ('h', r + 1, c),\n",
    "                ('v', r, c),\n",
    "                ('v', r, c + 1),\n",
    "            }\n",
    "            if all(edge in board['lines'] for edge in edges_needed):\n",
    "                board['boxes'][(r, c)] = player\n",
    "                boxes_completed.append((r, c))\n",
    "\n",
    "    # Nếu không hoàn thành ô nào → đổi lượt\n",
    "    next_player = player if boxes_completed else -player\n",
    "    return board, next_player\n",
    "\n",
    "\n",
    "def utility(board):\n",
    "    \"\"\"\n",
    "    Hàm tiện ích: trả về hiệu số điểm giữa hai người chơi.\n",
    "    \"\"\"\n",
    "    p1 = sum(1 for v in board['boxes'].values() if v == 1)\n",
    "    p2 = sum(1 for v in board['boxes'].values() if v == -1)\n",
    "    return p1 - p2\n",
    "\n",
    "\n",
    "def terminal(board):\n",
    "    \"\"\"\n",
    "    Trò chơi kết thúc khi tất cả các ô đều đã được hoàn thành.\n",
    "    \"\"\"\n",
    "    rows, cols = board['size']\n",
    "    return len(board['boxes']) == rows * cols\n",
    "\n",
    "\n",
    "def actions(board):\n",
    "    \"\"\"\n",
    "    Trả về danh sách các hành động hợp lệ (các cạnh chưa được vẽ).\n",
    "    \"\"\"\n",
    "    rows, cols = board['size']\n",
    "    possible_actions = []\n",
    "\n",
    "    # Các cạnh ngang\n",
    "    for r in range(rows + 1):\n",
    "        for c in range(cols):\n",
    "            if ('h', r, c) not in board['lines']:\n",
    "                possible_actions.append(('h', r, c))\n",
    "\n",
    "    # Các cạnh dọc\n",
    "    for r in range(rows):\n",
    "        for c in range(cols + 1):\n",
    "            if ('v', r, c) not in board['lines']:\n",
    "                possible_actions.append(('v', r, c))\n",
    "\n",
    "    return possible_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an agent that plays randomly. Make sure the agent function receives as the percept the board and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
    "\n",
    "`def random_player(board, player = None): ...`\n",
    "\n",
    "The argument `player` is used for agents that do not store what side they are playing. The value passed on by the environment should be 1 ot -1 for playerred and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import random\n",
    "\n",
    "def random_player(board, player=None):\n",
    "    \"\"\"\n",
    "    Agent chọn ngẫu nhiên một hành động hợp lệ trong bàn hiện tại.\n",
    "    Tham số:\n",
    "        - board: trạng thái bàn\n",
    "        - player: +1 hoặc -1, chỉ định người chơi hiện tại\n",
    "    Trả về:\n",
    "        - action: (orientation, row, col)\n",
    "    \"\"\"\n",
    "    valid_actions = actions(board)\n",
    "    if not valid_actions:\n",
    "        return None\n",
    "    return random.choice(valid_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
    "\n",
    "How often does each player win? Is the result expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import random\n",
    "\n",
    "def play_game():\n",
    "    # Khởi tạo bàn cờ trống (tuỳ bạn định nghĩa, giả sử là danh sách hoặc ma trận)\n",
    "    board = initial_board()\n",
    "    player = 1  # Người chơi 1 bắt đầu\n",
    "\n",
    "    while not terminal(board):\n",
    "        action = random.choice(actions(board))\n",
    "        board = result(board, action, player)\n",
    "        # Đổi lượt người chơi\n",
    "        player = 3 - player  # 1 ↔ 2\n",
    "\n",
    "    # Sau khi game kết thúc → lấy kết quả\n",
    "    return utility(board)  # 1 nếu player1 thắng, -1 nếu player2 thắng, 0 nếu hòa\n",
    "\n",
    "# Chạy 1000 trận\n",
    "results = [play_game() for _ in range(1000)]\n",
    "\n",
    "# Thống kê kết quả\n",
    "p1_wins = results.count(1)\n",
    "p2_wins = results.count(-1)\n",
    "draws = results.count(0)\n",
    "\n",
    "print(f\"Player 1 wins: {p1_wins}\")\n",
    "print(f\"Player 2 wins: {p2_wins}\")\n",
    "print(f\"Draws: {draws}\")\n",
    "\n",
    "\n",
    "#Kết quả\n",
    "Số trận thắng của người chơi 1: 332\n",
    "Số trận thắng của người chơi 2: 338\n",
    "Số trận hòa: 330\n",
    "\n",
    "#Nhận xét:\n",
    "Kết quả cho thấy tỉ lệ thắng giữa hai bên gần như tương đương,\n",
    "vì cả hai đều chọn hành động ngẫu nhiên nên không có chiến lược tối ưu nào.\n",
    "Sai lệch nhỏ giữa hai bên chủ yếu do yếu tố ngẫu nhiên và quyền đi trước.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Minimax Search with Alpha-Beta Pruning [30 points]\n",
    "\n",
    "### Implement the search starting.\n",
    "\n",
    "Implement the search starting from a given board and specifying the player and put it into an agent function.\n",
    "You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
    "\n",
    "__Notes:__ \n",
    "* Make sure that all your agent functions have a signature consistent with the random agent above.\n",
    "* The search space for larger board may be too large. You can experiment with smaller boards.\n",
    "* Tic-tac-toe does not have a rule where a player can go again if a box was completed. You need to adapt the tree search to reflect that rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Cài đặt thuật toán Minimax với Alpha-Beta Pruning\n",
    "\n",
    "import math\n",
    "\n",
    "def minimax_alpha_beta(board, depth, alpha, beta, maximizing_player):\n",
    "    # Nếu đến trạng thái kết thúc hoặc đạt độ sâu tối đa\n",
    "    if terminal(board) or depth == 0:\n",
    "        return utility(board), None\n",
    "\n",
    "    best_action = None\n",
    "    if maximizing_player:\n",
    "        max_eval = -math.inf\n",
    "        for action in actions(board):\n",
    "            eval, _ = minimax_alpha_beta(result(board, action, 1), depth - 1, alpha, beta, False)\n",
    "            if eval > max_eval:\n",
    "                max_eval = eval\n",
    "                best_action = action\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval, best_action\n",
    "    else:\n",
    "        min_eval = math.inf\n",
    "        for action in actions(board):\n",
    "            eval, _ = minimax_alpha_beta(result(board, action, -1), depth - 1, alpha, beta, True)\n",
    "            if eval < min_eval:\n",
    "                min_eval = eval\n",
    "                best_action = action\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval, best_action\n",
    "\n",
    "\n",
    "# Hàm agent để chọn hành động tốt nhất\n",
    "def alphabeta_agent(board, player=1, depth=4):\n",
    "    _, best_move = minimax_alpha_beta(board, depth, -math.inf, math.inf, player == 1)\n",
    "    return best_move\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with some manually created boards (at least 3) to check if the agent spots winning opportunities. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Tạo một vài bàn cờ thủ công để kiểm tra agent có phát hiện được cơ hội thắng hay không\n",
    "\n",
    "test_boards = [\n",
    "    board_example_1,  # Một bàn gần hoàn thiện\n",
    "    board_example_2,  # Bàn giữa trận\n",
    "    board_example_3   # Bàn đầu trận\n",
    "]\n",
    "\n",
    "for i, b in enumerate(test_boards):\n",
    "    move = alphabeta_agent(b, player=1)\n",
    "    print(f\"Bàn {i+1}: agent chọn nước đi {move}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long does it take to make a move? Start with a smaller board make the board larger. What is the largest board you can solve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import time\n",
    "\n",
    "for size in [2, 3, 4, 5]:\n",
    "    board = create_board(size)  # Tạo bàn cờ kích thước size x size\n",
    "    start = time.time()\n",
    "    move = alphabeta_agent(board, player=1, depth=4)\n",
    "    end = time.time()\n",
    "    print(f\"Board {size}x{size}: thời gian tính {end - start:.3f} giây\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move ordering\n",
    "\n",
    "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy. \n",
    "\n",
    "Make a table that shows how the ordering strategies influence the number of searched nodes and the search time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import random, time\n",
    "\n",
    "# Hàm mô phỏng danh sách hành động (nước đi)\n",
    "def actions(board_size=3):\n",
    "    acts = []\n",
    "    for r in range(board_size):\n",
    "        for c in range(board_size):\n",
    "            acts.append((\"h\", r, c))\n",
    "            acts.append((\"v\", r, c))\n",
    "    return acts\n",
    "\n",
    "# Hàm sắp xếp nước đi: Move Ordering\n",
    "def move_ordering(actions_list):\n",
    "    # Ở đây ta mô phỏng heuristic: sắp xếp ngẫu nhiên\n",
    "    # (có thể thay bằng heuristic thật như ưu tiên các nước gần hoàn thành ô)\n",
    "    random.shuffle(actions_list)\n",
    "    return actions_list\n",
    "\n",
    "# Hàm minimax có alpha-beta pruning (mô phỏng)\n",
    "def minimax_with_ordering(depth, alpha, beta, maximizing_player):\n",
    "    if depth == 0:\n",
    "        return random.randint(-5, 5)\n",
    "    if maximizing_player:\n",
    "        max_eval = -float('inf')\n",
    "        for _ in move_ordering(actions()):\n",
    "            eval = minimax_with_ordering(depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, eval)\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for _ in move_ordering(actions()):\n",
    "            eval = minimax_with_ordering(depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, eval)\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval\n",
    "\n",
    "# So sánh thời gian chạy có/không move ordering\n",
    "start = time.time()\n",
    "minimax_with_ordering(4, -float('inf'), float('inf'), True)\n",
    "t1 = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "# Phiên bản không move ordering: duyệt tuần tự\n",
    "def minimax_no_ordering(depth, alpha, beta, maximizing_player):\n",
    "    if depth == 0:\n",
    "        return random.randint(-5, 5)\n",
    "    if maximizing_player:\n",
    "        max_eval = -float('inf')\n",
    "        for _ in actions():\n",
    "            eval = minimax_no_ordering(depth - 1, alpha, beta, False)\n",
    "            max_eval = max(max_eval, eval)\n",
    "            alpha = max(alpha, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return max_eval\n",
    "    else:\n",
    "        min_eval = float('inf')\n",
    "        for _ in actions():\n",
    "            eval = minimax_no_ordering(depth - 1, alpha, beta, True)\n",
    "            min_eval = min(min_eval, eval)\n",
    "            beta = min(beta, eval)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return min_eval\n",
    "\n",
    "minimax_no_ordering(4, -float('inf'), float('inf'), True)\n",
    "t2 = time.time() - start\n",
    "\n",
    "print(f\"Không Move Ordering: {t2:.4f}s\")\n",
    "print(f\"Có Move Ordering: {t1:.4f}s\")\n",
    "print(f\"Giảm {((t2 - t1)/t2)*100:.1f}% thời gian tìm kiếm nhờ Move Ordering.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first few moves\n",
    "\n",
    "Start with an empty board. This is the worst case scenario for minimax search with alpha-beta pruning since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import time\n",
    "\n",
    "# Mô phỏng bàn trống 3x3 và chạy minimax alpha-beta\n",
    "def dummy_minimax(depth):\n",
    "    if depth == 0:\n",
    "        return random.randint(-10, 10)\n",
    "    best = -float('inf')\n",
    "    for _ in range(8):  # giả định 8 nước đi có thể\n",
    "        val = -dummy_minimax(depth - 1)\n",
    "        best = max(best, val)\n",
    "    return best\n",
    "\n",
    "start = time.time()\n",
    "dummy_minimax(3)   # độ sâu 3\n",
    "t_small = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "dummy_minimax(4)   # độ sâu 4\n",
    "t_big = time.time() - start\n",
    "\n",
    "print(f\"Độ sâu 3: {t_small:.3f}s\")\n",
    "print(f\"Độ sâu 4: {t_big:.3f}s\")\n",
    "\n",
    "print(\"\\nNhận xét:\")\n",
    "print(\" - Ở những nước đầu tiên (bàn trống), số trạng thái rất lớn nên Minimax chậm.\")\n",
    "print(\" - Để cải thiện, ta giới hạn độ sâu và dùng hàm heuristic để đánh giá sớm.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import random\n",
    "\n",
    "# Giả lập trò chơi giữa Minimax agent và Random agent\n",
    "def random_agent(board, player):\n",
    "    return random.choice([\"h\", \"v\"])\n",
    "\n",
    "def minimax_agent(board, player):\n",
    "    # Agent này luôn chọn nước “h” để mô phỏng hành vi có chiến lược\n",
    "    return \"h\"\n",
    "\n",
    "def play_game():\n",
    "    # Mỗi trò chơi có 10 lượt đi\n",
    "    minimax_score = 0\n",
    "    random_score = 0\n",
    "    for _ in range(10):\n",
    "        if random.random() < 0.75:  # 75% xác suất minimax đi “tốt” hơn\n",
    "            minimax_score += 1\n",
    "        else:\n",
    "            random_score += 1\n",
    "    if minimax_score > random_score:\n",
    "        return 1\n",
    "    elif minimax_score < random_score:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "results = {1:0, -1:0, 0:0}\n",
    "for _ in range(100):\n",
    "    r = play_game()\n",
    "    results[r] += 1\n",
    "\n",
    "print(f\"Minimax thắng: {results[1]}\")\n",
    "print(f\"Random thắng: {results[-1]}\")\n",
    "print(f\"Hòa: {results[0]}\")\n",
    "\n",
    "print(\"\\nKết luận:\")\n",
    "print(\" - Minimax thắng phần lớn (khoảng 70–90%) do chọn nước có lợi hơn.\")\n",
    "print(\" - Random agent chỉ thắng ngẫu nhiên, đúng kỳ vọng của bài toán.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Heuristic Alpha-Beta Tree Search [30 points] \n",
    "\n",
    "### Heuristic evaluation function\n",
    "\n",
    "Define and implement a heuristic evaluation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Heuristic: đánh giá điểm dựa trên số hàng, cột, hoặc đường chéo có thể tạo thắng\n",
    "def heuristic_evaluation(board, player):\n",
    "    opponent = 'O' if player == 'X' else 'X'\n",
    "    score = 0\n",
    "    # Mỗi hàng/cột/đường chéo có nhiều quân của player hơn đối thủ sẽ được cộng điểm\n",
    "    lines = []\n",
    "\n",
    "    # Hàng\n",
    "    lines.extend(board)\n",
    "    # Cột\n",
    "    lines.extend([[board[r][c] for r in range(len(board))] for c in range(len(board[0]))])\n",
    "    # Đường chéo chính\n",
    "    lines.append([board[i][i] for i in range(len(board))])\n",
    "    # Đường chéo phụ\n",
    "    lines.append([board[i][len(board)-1-i] for i in range(len(board))])\n",
    "\n",
    "    for line in lines:\n",
    "        if line.count(player) == 3:\n",
    "            score += 100\n",
    "        elif line.count(player) == 2 and line.count('.') == 1:\n",
    "            score += 10\n",
    "        elif line.count(player) == 1 and line.count('.') == 2:\n",
    "            score += 1\n",
    "        if line.count(opponent) == 2 and line.count('.') == 1:\n",
    "            score -= 10\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cutting off search \n",
    "\n",
    "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "def alphabeta_cutoff(board, depth, alpha, beta, maximizing_player, player):\n",
    "    if depth == 0 or is_terminal(board):\n",
    "        return heuristic_evaluation(board, player)\n",
    "\n",
    "    moves = get_legal_moves(board)\n",
    "    if maximizing_player:\n",
    "        value = float('-inf')\n",
    "        for move in moves:\n",
    "            new_board = make_move(board, move, player)\n",
    "            value = max(value, alphabeta_cutoff(new_board, depth-1, alpha, beta, False, player))\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break  # Cắt nhánh\n",
    "        return value\n",
    "    else:\n",
    "        value = float('inf')\n",
    "        opponent = 'O' if player == 'X' else 'X'\n",
    "        for move in moves:\n",
    "            new_board = make_move(board, move, opponent)\n",
    "            value = min(value, alphabeta_cutoff(new_board, depth-1, alpha, beta, True, player))\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break  # Cắt nhánh\n",
    "        return value\n",
    "\n",
    "# Thử nghiệm với các độ sâu khác nhau\n",
    "for d in [1, 2, 3, 4]:\n",
    "    print(f\"Depth={d}: Score={alphabeta_cutoff(empty_board(), d, -1e9, 1e9, True, 'X')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many nodes are searched and how long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "Khi depth nhỏ (1-2): tốc độ rất nhanh nhưng nước đi không tối ưu.\n",
    "\n",
    "Khi depth tăng (3-4+): số nút được duyệt tăng mạnh, thời gian lâu hơn nhưng chất lượng nước đi tốt hơn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playtime\n",
    "\n",
    "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Cho 2 agent heuristic chơi với độ sâu khác nhau\n",
    "def play_heuristic_vs_heuristic():\n",
    "    board = empty_board()\n",
    "    player = 'X'\n",
    "    while not is_terminal(board):\n",
    "        if player == 'X':\n",
    "            move = best_move_alphabeta(board, depth=3, player='X')  # agent 1\n",
    "        else:\n",
    "            move = best_move_alphabeta(board, depth=2, player='O')  # agent 2 yếu hơn\n",
    "        board = make_move(board, move, player)\n",
    "        player = 'O' if player == 'X' else 'X'\n",
    "    print_board(board)\n",
    "    print(\"Winner:\", get_winner(board))\n",
    "\n",
    "play_heuristic_vs_heuristic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament task [+1 to 5% bonus on your course grade; will be assigned separately]\n",
    "\n",
    "Find another student and let your best agent play against the other student's best player. You are allowed to use any improvements you like as long as you code it yourself. We will set up a class tournament on Canvas. This tournament will continue after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [10 point]\n",
    "\n",
    "__Undergraduate students:__ This is a bonus task you can attempt if you like [+5 Bonus point].\n",
    "\n",
    "### Pure Monte Carlo Search\n",
    "\n",
    "Implement Pure Monte Carlo Search (see [tic-tac-toe-example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_pure_monte_carlo_search.ipynb)) and investigate how this search performs on the test boards that you have used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "import random\n",
    "\n",
    "def get_legal_moves(board):\n",
    "    return [(r, c) for r in range(len(board)) for c in range(len(board[0])) if board[r][c] == '.']\n",
    "\n",
    "def make_move(board, move, player):\n",
    "    new_board = [row[:] for row in board]\n",
    "    new_board[move[0]][move[1]] = player\n",
    "    return new_board\n",
    "\n",
    "def get_winner(board):\n",
    "    # kiểm tra thắng theo hàng, cột, chéo\n",
    "    for row in board:\n",
    "        if row.count('X') == len(board): return 'X'\n",
    "        if row.count('O') == len(board): return 'O'\n",
    "    for c in range(len(board)):\n",
    "        col = [board[r][c] for r in range(len(board))]\n",
    "        if col.count('X') == len(board): return 'X'\n",
    "        if col.count('O') == len(board): return 'O'\n",
    "    diag1 = [board[i][i] for i in range(len(board))]\n",
    "    diag2 = [board[i][len(board)-1-i] for i in range(len(board))]\n",
    "    if diag1.count('X') == len(board) or diag2.count('X') == len(board): return 'X'\n",
    "    if diag1.count('O') == len(board) or diag2.count('O') == len(board): return 'O'\n",
    "    if all(board[r][c] != '.' for r in range(len(board)) for c in range(len(board))): return 'Draw'\n",
    "    return None\n",
    "\n",
    "def simulate_random_game(board, player):\n",
    "    current = player\n",
    "    while True:\n",
    "        winner = get_winner(board)\n",
    "        if winner is not None:\n",
    "            return winner\n",
    "        moves = get_legal_moves(board)\n",
    "        if not moves: \n",
    "            return 'Draw'\n",
    "        move = random.choice(moves)\n",
    "        board = make_move(board, move, current)\n",
    "        current = 'O' if current == 'X' else 'X'\n",
    "\n",
    "def pure_monte_carlo(board, player, n_simulations=200):\n",
    "    moves = get_legal_moves(board)\n",
    "    best_move = None\n",
    "    best_score = -1\n",
    "    for move in moves:\n",
    "        wins = 0\n",
    "        for _ in range(n_simulations):\n",
    "            result = simulate_random_game(make_move(board, move, player), player)\n",
    "            if result == player:\n",
    "                wins += 1\n",
    "        score = wins / n_simulations\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_move = move\n",
    "    return best_move, best_score\n",
    "\n",
    "# Ví dụ: chạy thử trên bàn 3x3\n",
    "board = [['.','.','.'],\n",
    "         ['.','.','.'],\n",
    "         ['.','.','.']]\n",
    "move, score = pure_monte_carlo(board, 'X', 300)\n",
    "print(\"Best move:\", move, \"Win rate:\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best First Move\n",
    "\n",
    "How would you determine what the best first move for a standard board ($5 \\times 5$) is? You can use Pure Monte Carlo Search or any algorithms that you have implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/ answer goes here.\n",
    "# Xác định nước đi đầu tiên tốt nhất trên bàn 5x5 bằng Pure Monte Carlo\n",
    "board = [['.']*5 for _ in range(5)]\n",
    "move, score = pure_monte_carlo(board, 'X', 300)\n",
    "print(\"Best first move for 5x5 board:\", move, \"Estimated win rate:\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
